\documentclass[10pt,journal]{IEEEtran}

\usepackage{bm,amsmath}
\usepackage{subfigure,epsfig,graphicx}
\usepackage{verbatim}
\usepackage{cite}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{color}
\usepackage{multirow}




\begin{document}

\title{Report for Relational Learning}
\author{ Yu Wu, Nov. 2015
             }
\maketitle



\section{Proposed Model}
The knowledge graph data $\mathcal{D}$ consists of a set of links between a fixed set of entities. Let $\mathcal{E} = \{e_1, \ldots, e_{N_e}\}$ denote the entity set and $\mathcal{R} = \{r_1, \ldots, r_{N_r}\}$ the link set.  The relationship between entities indicated by $\mathcal{D}$ are represented as relation triples such as $(e_h,r_{\ell},e_t)$, where $e_h, e_t\in \mathcal{E}$ are referred as the head  and tail, respectively, and  $r_{\ell} \in \mathcal{R}$ the link.  For example, $(\emph{Champa}, \emph{formofgoverment}, \emph{Monarchy})$ is one of such relation triples which the head entity \emph{Champa} and the tail entity \emph{Monarchy} is linked by the relation type \emph{formofgoverment}.  For the sake of convenience of discussion, we simplified the relation triple as $(h,\ell, t)$   by referring only the indices of the entities and links. Given a set of known links within $\mathcal{D}$, the goal of a relational learning task is to  infer unknown links and correct the known links to complete the graph $\mathcal{D}$. One way to solve this task is to learn an energy function $E(h, \ell, t)$ on the set of all poosible triples $\mathcal{E} \times \mathcal{R} \times \mathcal{E}$ so that a triple representing a truly existing link between two entities  is assembled with a low energy, otherwise with high energy. 

%For instance, $E(h, l, t)\geq c$ indicates there exists link $r_l$ between the entities $e_h$ and $e_t$ and $E(h, l, t)<c$ otherwise, where $c$ is a threshold. 

\subsection{Energy Function}
The proposed method  \emph{tranPES}  parameterises the energy function of an input relation triple over three individual $k$-dimensional embedding vectors of its head, tail and link, as well as as a set of $k\times k$ transformation matrices $\{\textbf{P}_{ht}\}_{h,t}$ where different matrices are constructed for different head-tail entity pairs $(h,t)$. Letting $\{\bm e_i\}_{i=1}^{N_e}$ denote the entity embedding vectors and  $\{\bm r_i\}_{i=1}^{N_r}$ the link embeddings, the \emph{tranPES} energy function is defined as the following:
\begin{equation} \label{energy}
\mathrm{E}{(h,\ell, t)} = \|\bm{e}_h +\textbf{P}_{ht} \bm{r}_{\ell}-\bm{e}_t\|_2.
\end{equation}
Apart from the $l_2$-norm, it is  possible to compute the energy based on other norms, e.g., the $l_1$-norm, $p$-norm, or other dissimilarity measures. Instead of freely setting the transformation matrices, $\textbf{P}_{ht}$ is restricted as a projection matrix that  gives the projection of a $k$-dimensional vector onto the space spanned by the two $k$-dimensional entity vectors $\bm{e}_h$ and $\bm{e}_t$.  Letting the columns of the $k\times 2$ matrix $\textbf{E}_{ht}$ store the two entity embedding vectors $\bm{e}_h $ and $\bm{e}_t$, the projection matrix is given by 
\begin{equation}
\textbf{P}_{ht}=\textbf{E}_{ht}\left(\textbf{E}_{ht}^T \textbf{E}_{ht} \right)^{-1}\textbf{E}_{ht}^T.
\end{equation}
To deal with cases when $\bm{e}_h$ and $\bm{e}_t$ take the same direction, the above expression can be further modified by adding a  regularisation term, leading to
\begin{equation}
\textbf{P}_{ht}=\textbf{E}_{ht}\left(\textbf{E}_{ht}^T \textbf{E}_{ht} +\xi\textbf{I}\right)^{-1}\textbf{E}_{ht}^T.
\end{equation}
The orthogonal complementary $(\textbf{I} -  \textbf{P}_{ht}) \bm{r}_{\ell}$ of the projected link embedding vector  is always close to perpendicular to  $\bm{e}_h$ and $\bm{e}_t$. This can be shown by the following:
\begin{align}
 &\begin{bmatrix} \bm{e}_h^{T} \\ \bm{e}_t^{T} \end{bmatrix} (\textbf{I} -  \textbf{P}_{ht})\bm{r}_{\ell} \notag  \\
=&\textbf{E}_{ht}^{T}\left(\textbf{I} -  \textbf{P}_{ht}\right)\bm{r}_{\ell}  \notag \\
=&\textbf{E}_{ht}^{T}\left(\bm{r}_{\ell} -  \textbf{E}_{ht}\left(\textbf{E}_{ht}^{T} \textbf{E}_{ht} + \xi \textbf{I} \right)^{-1} \textbf{E}_{ht}^{T} \bm{r}_{\ell}\right)  \notag \\
=&\textbf{E}_{ht}^{T}\bm{r}_{\ell} - \left (\textbf{E}_{ht}^{T} \textbf{E}_{ht}+\xi \textbf{I} - \xi \textbf{I}\right)\left(\textbf{E}_{ht}^{T} \textbf{E}_{ht} + \xi \textbf{I}\right )^{-1} \textbf{E}_{ht}^{T} \bm{r}_{\ell} \notag \\
=& \textbf{E}_{ht}^{T}\bm{r}_{\ell} - \textbf{E}_{ht}^{T} \bm{r}_{\ell}  +  \xi \left(\textbf{E}_{ht}^{T} \textbf{E}_{ht} + \xi \textbf{I}\right )^{-1} \textbf{E}_{ht}^{T} \bm{r}_{\ell}  \notag \\
=& \xi \left(\textbf{E}_{ht}^{T} \textbf{E}_{ht} + \xi \textbf{I}\right )^{-1} \textbf{E}_{ht}^{T} \bm{r}_{\ell}   \label{temp}
\end{align}

Let the economic singular value decomposition of the $k \times 2$ matrix $\textbf{E}_{ht}$ as:
\begin{equation*}
\textbf{E}_{ht} = \textbf{U} \mathbf{\Sigma} \textbf{V}^{T}
\end{equation*}
where \textbf{U} is a $k \times 2$ real unitary matrix, $\mathbf{\Sigma}$ is a $2 \times 2$ rectangular diagonal matrix with non-negative real numbers on the diagonal, and \textbf{V} is a $2 \times 2$ real unitary matrix.

Then the Eqs. (\ref{temp}) can be further simplified as:
\begin{align}
& \xi \left(\textbf{E}_{ht}^{T} \textbf{E}_{ht} + \xi \textbf{I}\right )^{-1} \textbf{E}_{ht}^{T} \bm{r}_{\ell} \notag \\
=& \xi \left(\textbf{V} \mathbf{\Sigma} \textbf{U}^{T}  \textbf{U} \mathbf{\Sigma} \textbf{V}^{T} + \xi \textbf{I}\right )^{-1} \textbf{V} \mathbf{\Sigma} \textbf{U}^{T} \bm{r}_{\ell} \notag \\
=& \xi \left(\textbf{V} \mathbf{\Sigma}^2 \textbf{V}^{T} + \xi \textbf{I}\right )^{-1} \textbf{V} \mathbf{\Sigma} \textbf{U}^{T} \bm{r}_{\ell} \notag \\
=& \xi \textbf{V} \left(\mathbf{\Sigma}^2 + \xi \textbf{I} \right)^{-1} \textbf{V}^{T} \textbf{V} \mathbf{\Sigma} \textbf{U}^{T} \bm{r}_{\ell} \notag \\
=& \xi \textbf{V} \left(\mathbf{\Sigma}^2 + \xi \textbf{I} \right)^{-1} \mathbf{\Sigma} \textbf{U}^{T} \bm{r}_{\ell} \notag \\
=& \xi \left( \textbf{V} \begin{bmatrix} \frac{\sigma_1}{\sigma_1^2 + \xi} & 0 \\ 0 & \frac{\sigma_2}{\sigma_2^2 + \xi}   \end{bmatrix} \textbf{U}^{T} \bm{r}_{\ell}  \right)\notag \\
\approx & \bm{0}
\end{align}

Thus, the regularisation term $\xi \textbf{I}$ not only making the projection transformation matrices $\{\textbf{P}_{ht}\}_{h,t}$ calculable in all circumstances but also preserves the property that the projected link embedding vector  could only be spanned by the corresponding entity embedding vectors.

\subsection{Training}
Given a set of known links between entities, a set of valid triples can be created from it. This set is referred as the positive triple  set, denoted as $\mathcal{D}^+$. For each positive triple $(h,l,t)\in\mathcal{D}^+$, a set of  corrupted triples can be generated by replacing either its head or tail  with a different one, given as
\begin{align*}
\small
\mathcal{D}^{-}_{h,l,t} = &  \left\{ (h^{'},\ell,t)  |  h^{'} \in \{ 1,2,\ldots, N_e\}, (h^{'},\ell,t)\notin \mathcal{D}^+ \right\} \cup \\
 &  \left\{ (h,\ell,t^{'})  |   t^{'} \in \{ 1,2,\ldots, N_e\},  (h,\ell,t^{'}) \notin \mathcal{D}^+\right\}
\end{align*}

To learn the energy function in Eq. (\ref{energy}) parameterised on the entity and link embeddings is equivalent to the optimisation of these embedding vectors to encourage the maximum discrimination between  the positive  and negative triples. To achieve this, a margin-based ranking loss can be employed, given as
\begin{equation} \label{loss}
\small
\mathcal{L}_m = \sum_{(h,\ell,t)\in \mathcal{D}^+} \sum_{(h^{'},\ell,t^{'})\in  \mathcal{D}^{-}_{h,\ell,t}} [\gamma + \mathrm{E}(h,\ell,t)- \mathrm{E}(h^{'},\ell,t^{'}) ]_+,
\end{equation}
where $[*]_+$ denotes the positive part of the input number $*$, and $\gamma > 0$ is a user-set margin parameter.

A length constraint can be imposed to the entity embeddings to prevent the training process to trivially minimise $\mathcal{L}_m$ by artificially increasing the scale of the entity embedding. This constraint can be incorporated into the cost function $\mathcal{L}_m$.  And we add the regularisation term  for the relational embedding vectors. This leads to a regularised cost function given as
\begin{equation} \label{loss2}
\mathcal{L} = \mathcal{L}_m  +\lambda_1 \sum_{i =1}^{N_e} \left[\|\bm{e}_i\|_2^2 - 1\right]_+ + \lambda_2 \sum_{j=1}^{N_r} \| \bm{r}_{j} \|_2^2,
\end{equation}
where $\lambda_1, \lambda_2 >0$ are the regularisation parameters. Finally, the  following optimisation problem is solved, given as
\begin{equation}
\min_{\{\bm e_i\}_{i=1}^{N_e}, \{\bm r_i\}_{i=1}^{N_r}} \mathcal{L}\left(\{\bm e_i\}_{i=1}^{N_e}, \{\bm r_i\}_{i=1}^{N_r}, \xi,\lambda_1, \lambda_2 \right).
\end{equation}

Similar to the optimisation procedure used in \cite{bordes_translating_2013}, a stochastic gradient descent algorithm in minibatch mode is used. The embedding vectors are first initialised following the random procedure in \cite{glorot_understanding_2010}. The algorithm is stopped based on its performance on a validation set.  Pseudo code of the proposed algorithm is provided in Algorithm $\textbf{1}$.


\begin{algorithm*}
\caption{Learning TranPES }
\textbf{input} Training set $\mathcal{D} = \{(h,\ell,t)\}$, entities and rel. set $\mathcal{E}$ and $\mathcal{R}$, margin $\gamma$, regularisation term $\lambda_1, \lambda_2$ and embeddings dimension $k$.
\begin{enumerate}
\item \textbf{initialise} 
\begin{description}
\item $\bm{r} \leftarrow$ uniform $(-\frac{6}{\sqrt{k}}, \frac{6}{k})$ for each $\bm{r} \in \mathcal{R}$ 
\item	$\bm{r} \leftarrow \bm{r} / \| \bm{r} \|$ for each $\bm{r} \in \mathcal{R}$
\item $\bm{e} \leftarrow$ uniform $(-\frac{6}{\sqrt{k}}, \frac{6}{k})$ for each $\bm{e} \in \mathcal{E}$ 
\item	$\bm{e} \leftarrow \bm{e} / \| \bm{e} \|$ for each $\bm{e} \in \mathcal{E}$
\end{description}

\item \textbf{loop}
\begin{description}
\item $\mathcal{D}_{batch} \leftarrow$ sample from $\mathcal{D}$ 	{\color{green} // sample a minibatch triplets of size b}
\item $\mathcal{T}_{batch} \leftarrow \emptyset$  {\color{green} //  for storing the set of pairs of correct and corrupted triplets}
\item \textbf{for} $(h,l,t) \in \mathcal{D}$ \textbf{do}
\begin{description}
\item $(h^{'},\ell,t^{'}) \leftarrow$ sample from $\mathcal{D}^{-}_{(h,l,t)}$ {\color{green} // sample a corrupted triplet}
\item $\mathcal{T}_{batch} \leftarrow \mathcal{T}_{batch} \cup \{ ((h,\ell,t),(h^{'},\ell,t^{'})) \}$
\end{description}
\item \textbf{end for}
\item $\mathcal{E}_{batch} \leftarrow$  head and tail set of $\mathcal{T}_{batch}$
\item $\mathcal{R}_{batch} \leftarrow$  link set of $\mathcal{T}_{batch}$ 
\item Updates embeddings w.r.t.  
\begin{equation*}
\sum_{(h, \ell, t),(h^{'},\ell,t^{'})\in  \mathcal{T}_{batch}} {[\gamma + \mathrm{E}(h,l,t)- \mathrm{E}(h^{'},\ell,t^{'}) ]_+}  +\lambda_1 \sum_{m \in \mathcal{E}_{batch}} \left[\|\bm{e}_m\|_2^2 - 1\right]_+ + \lambda_2 \sum_{n \in \mathcal{R}_{batch}} \| \bm{r}_{n} \|_2^2,
\end{equation*}

\end{description}

\end{enumerate}

\end{algorithm*}

\subsection{Discussion}
The proposed idea is inspired by the limitation of the energy function $\mathrm{E}{(h,\ell, t)} = \|\bm{e}_h + \bm{r}_{\ell}-\bm{e}_t\|_2^2$ of \emph{transE} \cite{bordes_translating_2013}  which is one of the most commonly used algorithm in relational learning.  For instances, considering the simple link of ``$isa$" in  the statement \emph{Bob Dylan was a song writer, singer, performer, book author and film actor}, the following relation triples can be generated from it:
\\
\begin{tabular}[center]{l l l} 
 \\
 \emph{head} & \emph{link} & \emph{tail} \\
 \hline 
 (\emph{BobDylan}, & \emph{isa}, & \emph{SongWriter}) \\
 (\emph{BobDylan}, & \emph{isa}, & \emph{Singer}) \\
  (\emph{BobDylan}, & \emph{isa}, & \emph{Performer}) \\
   (\emph{BobDylan}, & \emph{isa}, & \emph{BookAuthor }) \\
    (\emph{BobDylan}, & \emph{isa}, & \emph{FilmActor}) \\\\
 \end{tabular}\\
Both \emph{transE} and many existing algorithms assign one unified embedding vector to represent the same link.  Since the learned energies of the valid triples  possess low values,  different entities such as song writer, singer and performer are  distributed very close to each other in the embedding space due to their closeness to the same embedding vector of  the link ``$isa$". Although there exist distinctions between the occupations represented by these entities, the learned embeddings are not able to highlight such distinction. To overcome this,  the proposed energy function  first projects the link embedding vector onto the hyperplane spanned by the two  entity embedding vectors, and then performs the distance computation within the hyperplane.  This subsequently results in different representations of the link when it gets involved with different entity pairs.  The energy function  $\mathrm{E}{(h,\ell, t)} = \|\textbf{P}_l\bm{e}_h + \bm{r}_{\ell}-\textbf{P}_l\bm{e}_t\|_2^2$ of \emph{transR} \cite{lin_learning_2015} is another one proposed to overcome the limitation of   \emph{transE}. It allows the entities and links to be distributed in different embedding spaces of dimensions $d$ and $k$, respectively, and  introduces a set of $ k\times d$ projection matrices $\{\textbf{P}_l\}_{l=1}^{N_r}$ to align the two spaces over each link. In a way, it fixes an embedding space for the links, and project each pair of the entities $(\bm{e}_h, \bm{e}_t)$ onto this space to further formulate the distance. For the same entity,  its final representation $\textbf{P}_l\bm{e}_h$ used for comparing distance  differs over different links. On the contrary, the proposed method allows various customised representations $\textbf{P}_{ht} \bm{r}_{\l}$ for a link to suit the needs of different entity pairs, but maintains the fixed embedding representation for the entities when comparing distance. This thus allows the opportunity to propagate the relation information through entities. For instance, we assume there exist entities $c_1,c_2, \ldots,d_1,d_2,\ldots, e_1, e_2, \ldots$ belonging to three classes of $C,D,E$ and the class structure can be reflected by the link information.   When the entities are projected in a same fixed space,  their embeddings are able to  show directly the within-class closeness and between-class dispersion in the same space, thus it is possible to  transfer the instance-based inference to the class-based inference e.g., from $(c_i, r_1, d_j) \wedge (d_j, r_2,, e_k) \Rightarrow  (c_i, r_3, e_k)$ to $(C, r_1, D)\wedge (D, r_2, E) \Rightarrow  (C, r_3, E)$.



%This model also permit to propagate information between triples as each entity are having the same representation over all different relation types. For instance, suppose that in the embeddings there are entities $c_1,c_2, \ldots,d_1,d_2,\ldots, e_1, e_2, \ldots$ belong to 3 classes $C,D,E$, and the enitities have been embedded very close within each class. If there are some triples with the relationship that both $(c_i, r_1, d_j), (d_j, r_2,, e_k)$ and $(c_i, r_3, e_k)$ are true, then the deterministic rules in the graph that if $(C, r_1, D), (D, r_2, E)$ holds then $(C, r_3, E)$ have to be true are learned if no conflicting information has involved.

For evaluation, we use the same ranking procedure  as in \cite{bordes_learning_2011}. For each test triplet, the head is removed and replaced by each of the entities of the dictionary in turn. Dissimilarities (or energies) of those corrupted triplets are first computed by the models and then sorted by ascending order; the rank of the correct entity is finally stored. This whole procedure is repeated while removing the tail instead of the head. The \emph{mean} of those predicted ranks and the \emph{hit@10}, i.e. the proportion of correct entities ranked in the top $10$ are used for the evaluation.


\section{Experiments}
To compare our proposed algorithm with various other methods from the literature, we evaluate our approach on the two typical knowledge base, WordNet \cite{miller_wordnet:_1995} and Freebase \cite{bollacker_freebase:_2008}. We listed some statistics of these data in Table \ref{data}.

\subsection{Data sets}
\subsubsection{Wordnet} is a large lexical database of English. It groups words into sets of cognitive synonyms (synsets) and interlink these synsets by means of a small number of semantic relations, such as \emph{synonymy}, \emph{antonymy}, \emph{meronymy} and \emph{hypernymy}.  For example, a typical triplet \emph{(\_range\_NN\_3, \_hypernym, \_tract\_NN\_1)} means the third meaning of the noun "range"  is a hypernym of the first sense of the noun "tract". We consider the data \emph{WN18} used in \cite{bordes_semantic_2014}, which contains 18 relational types.

\subsubsection{Freebase} is a massive online collection database consisting of general human knowledge. It organised the human knowledge data  directly in the triplet form of \emph{(head, link, tail)}. Unlike the Wordnet database, it does not disambiguate the senses of each entity. Typical triplet example includes \emph{(Peasant Magic, /music/artist/album, Solipsitalism)}, \emph{(Barack Obama, /people/person/religion, Christianity)} and \emph{(Orange France, /organization/organization/place\_founded, Paris)}. FB15k \cite{bordes_learning_2011} is created by selecting the frequent occurrent entities and have been used for comparison with other algorithms in this paper.

\begin{table}[t]
\caption{Statistics of data sets} \label{data}
\centering
\begin{tabular}[center]{|l |l l |} 
 \hline
Dataset & WN18 & FB15k \\
 \hline 
Relationships & $18$ & $1,345$ \\
Entities & $40,943$ & $14,951$ \\
Train & $141,442$ & $483,142$ \\
Valid & $5,000$ & $50,000$ \\
Test & $5,000$ & $59,071$ \\
\hline
 \end{tabular}
 \end{table}
 
\subsection{Evaluation}
We compare the proposed algorithm with the state-of-art  methods on the WN18 and FB15k data set for link prediction. Essentially, every method trains a score function (or energy function in our case) to assemble the likely relations with a higher score (or lower energy) than the unlikely relations. This score function (energy function) corresponding to each model's trained parameters could thus gives its estimation of the likely score for every true triplets in the test set. Two evaluation metric \cite{bordes_learning_2011}, \emph{mean rank} and \emph{hits@10}, about measuring how well the predicted score matched with the true triple in the test set have been applied in our paper.

\subsubsection{mean rank} for each correct triplet in the test set, we first construct the corrupt triples by replacing the head entity with all the entities in knowledge base in turn. The score (or energy) of these corrupted triples are computed by the model and have been sorted in descending (or ascending) order. The rank of the correct head entity is finally stored. This procedure is repeated by replacing the tail entity of the each triplet in the test set. The mean of all these predicted ranks  in the test set have been used as the \emph{mean rank} assessment.

\subsubsection{hits@10} it is very biased by only utilising the \emph{mean rank} evaluation scheme. This is because that this evaluation scheme only cares about the dominant relationship related triples if the test data is heavily unbalanced with relation types, consequently,  other relation types' contribution to the \emph{mean rank} assessment have been ignored.  Thus, the proportion of the correct entities ranked within top $10$ have been reported as a supplemental evaluation indicator. In the model training, it is important to select the margin $\gamma$ for balancing the importance of each relationship, which could result a better $hits@10$ performance. 

The ranking of each entity within these evaluation metric can be inaccurate when some corrupted triples turns out to be the correct ones in the knowledge graph. \cite{bordes_translating_2013} suggest that filtering out the corrupted triples which appears either in the training, validation or test set before ranking those entities.  We labeled the first evaluation metric as \emph{raw} and the later filtered evaluation metric as \emph{filtered}.

\begin{table*}
\caption{Evaluation Results on WN18 and FB15k} \label{result1}
\centering
\begin{tabular}[center]{| c | c  c  | c c | c   c | c  c |} 
 \hline
Dataset & \multicolumn{4}{c}{WN18} & \multicolumn{4}{|c|}{FB15k} \\
\hline
\multirow{2}{*}{Metric} & \multicolumn{2}{ c |}{Mean Rank} & \multicolumn{2}{c |}{Hits@10(\%)} & \multicolumn{2}{c |}{Mean Rank}  &  \multicolumn{2}{c|}{Hits@10(\%)}   \\ 
\multicolumn{1}{|  c |}{} 	& 
Raw & Filter & Raw & Filter& Raw & Filter &  Raw & Filter \\
\hline
Unstructured & $315$ & $304$ & $35.3$ & $38.2$ & $1,074$ & $979$ & $4.5$ & $6.3$ \\
\hline
RESCAL & $1,180$ & $1,163$ & $37.2$ & $52.8$ & $828$ & $683$ & $28.4$ & $44.1$ \\
\hline
SE & $1,011$ & $985$ & $68.5$ & $80.5$ & $273$ & $162$ & $28.8$ & $39.8$ \\
\hline
SME(linear) & $545$ & $533$ & $65.1$ & $74.1$ & $274$ & $154$ & $30.7$ & $40.8$ \\
\hline
SME(bilinear) & $526$ & $509$ & $54.7$ & $61.3$ & $284$ & $158$ & $31.3$ & $41.3$ \\
\hline
LFM & $469$ & $456$ & $71.4$ & $81.6$ & $283$ & $164$ & $26.0$ & $33.1$ \\
\hline
TransE & $263$ & $251$ & $75.4$ & $89.2$ & $243$ & $125$ & $34.9$ & $47.1$ \\
\hline
TransH & $318$ & $303$ & $75.4$ & $86.7$ & $211$ & $84$ & $42.5$ & $58.5$ \\
\hline
TransR & $232$ & $219$ & $78.3$ & $91.7$ & $226$ & $78$ & $43.8$ & $65.5$ \\
\hline
CTransR & $243$ & $230$ & $78.9$ & $92.3$ & $233$ & $82$ & $44$ & $66.3$ \\
\hline
TranPES & $223$ & $NA$ & $71.6$ & $NA$ & $\mathbf{198}$ & $\mathbf{66}$ & $\mathbf{48.05}$ & $\mathbf{67.3}$ \\
\hline
\end{tabular}

\quad
\caption{Detailed Evaluation on FB15k} \label{result2}
\centering
\begin{tabular}{| c | c  c  | c c | c   c | c  c |} 
 \hline
Tasks & \multicolumn{4}{ c |}{Predicting Head(Hits@10)} & \multicolumn{4}{c |}{Predicting Tail(Hits@10)}  \\ 
\hline
Relation Category	& 
1-To-1 & 1-To-M & M-To-1 & M-To-M& 1-To-1 & 1-To-M &  M-To-1 & M-To-M \\
\hline
Unstructured & $34.5$ & $2.5$ & $6.1$ & $6.6$ & $34.3$ & $4.2$ & $1.9$ & $6.6$ \\
\hline
SE & $35.6$ & $62.6$ & $17.2$ & $37.5$ & $34.9$ & $14.6$ & $68.3$ & $41.3$ \\
\hline
SME(linear) & $35.1$ & $53.7$ & $19.0$ & $40.3$ & $32.7$ & $14.9$ & $61.6$ & $43.3$ \\
\hline
SME(bilinear) & $30.9$ & $69.6$ & $19.9$ & $38.6$ & $28.2$ & $13.1$ & $76.0$ & $41.8$ \\
\hline
TransE & $43.7$ & $65.7$ & $18.2$ & $47.2$ & $43.7$ & $19.7$ & $66.7$ & $50.0$ \\
\hline
TransH & $66.7$ & $81.7$ & $30.2$ & $57.4$ & $63.7$ & $30.1$ & $83.2$ & $60.8$ \\
\hline
TransR & $76.9$ & $77.9$ & $38.1$ & $66.9$ & $76.2$ & $38.4$ & $76.2$ & $69.1$ \\
\hline
CTransR & $\mathbf{78.6}$ & $77.8$ & $36.4$ & $\mathbf{68.0}$ & $77.4$ & $37.8$ & $78.0$ & $\mathbf{70.3}$ \\
\hline
TranPES & $78.0$ & $\mathbf{88.6}$ & $\mathbf{38.9}$ & $67.3$ & $\mathbf{78.9}$ & $\mathbf{42.1}$ & $\mathbf{84.2}$ & $69.8$ \\
\hline
\end{tabular}

\end{table*}


\subsection{Results}
In the experimental phase, we direct compare our proposed models with those results reported in the literature  \cite{lin_learning_2015} as we are using the same data sets. For the experiments of the TranPES algorithm, we first identify the scale of each parameters by searching the learning rate among $\{ 0.1, 0.01, 0.002\}$, the dimensions of the entity and link embedding $k$ among $\{ 20, 50, 100 \}$, the scaling control factor $\lambda_1$ is assigned as a constant value $1$, batchsize $B$ among $\{50, 100, 200\}$, the regularisation parameter $\lambda_2$ among $\{ 1, 0.1, 0.01\}$ and the margin $\gamma$ among $\{0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}$. The value of margin $\gamma$ is closely related to the $hits@10$ evaluation metric thus it is important to do a full searching for this parameter. For both datasets, we limit the epochs round at most 1000 times.  And we select the best model according to the best mean rank on the validation set.

The optimal configuration were: $k=20, B = 100, \lambda_2=0.01, \gamma=0.7$ on WN18 and $k=100, B=100, \lambda_2 = 0.01, \gamma=0.4$ on FB15k. The evaluation results are present on Table. \ref{result1}.

On the WN18 data set, it seems that the proposed algorithm does not perform better than even its simpler form of TransE algorithm. While on the more complex data set FB15k, the TranPES outperforms all the other algorithms.  The performances of TransR and TranPES is close on the FB15k, but the TranPES algorithm has much less parameters for the training(only $O(N_ek+N_rk)$).

We followed the same detailed evaluation protocal proposed in \cite{bordes_translating_2013}. It classifies the $hits@10$ results according to the four categories each relationship belong to:  1-To-1, 1-To-Many, Many-To-1, Many-To-Many. For instance, a given relationship is classified in to 1-To-Many if a \emph{head} can appear with at many \emph{tails} and a \emph{tail} can appear with at most one \emph{head}.  The corresponding result was shown on Table. \ref{result2}. 

As one would expect, it is approachable to predict \emph{head} in the 1-To-1, 1-To-Many  relationships and predict \emph{tail} in the 1-To-1, Many-To-1 relationship. This has been shown on the TranPES results subsequently. Upon successfully predicting these relationships, it shows the reasoning ability of TranPES algorithm.







\bibliographystyle{IEEEtran}
\bibliography{My_Library}

\end{document} 