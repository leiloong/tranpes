\documentclass[9pt]{sig-alternate-05-2015}

\usepackage{bm,amsmath}
\usepackage{subfigure,epsfig,graphicx}
\usepackage{verbatim}
\usepackage{cite}
\usepackage{url}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{color}
\usepackage{multirow}
\usepackage{caption} 

\DeclareMathOperator*{\argmin}{\arg\!\min}
\captionsetup[table]{skip=2pt}




\begin{document}

% Copyright
\setcopyright{acmcopyright}

\title{Translating on Pairwise Entity Space for Knowledge Graph Embedding}

%
% --- Author Metadata here ---
\numberofauthors{3}
% 1st,author
\author{
\alignauthor
Yu Wu \\
       \affaddr{The University of Liverpool}\\
       \affaddr{Brownlow Hill}\\
       \affaddr{ Liverpool, UK}\\
       \email{yuwu@liverpool.ac.uk}
 % 2nd. author
\alignauthor
Tingting Mu \\
       \affaddr{The University of Liverpool}\\
        \affaddr{Brownlow Hill}\\
       \affaddr{Liverpool, UK}\\
       \email{t.mu@liverpool.ac.uk}
 % 3rd. author
 \alignauthor
John Yannis Goulermas \\
       \affaddr{The University of Liverpool}\\
        \affaddr{Brownlow Hill}\\
       \affaddr{Liverpool, UK}\\
       \email{j.y.goulermas@liverpool.ac.uk}
 }
\date{11 Feb 2016}
             
\maketitle

\begin{abstract}
Knowledge graphs, which involve multiple types of relations between entities or attributes, are very important resources for AI related tasks, such as  word-sense disambiguation, information retrieval and question answering. In this paper, we have studied the problem of embedding the entities and relations of the knowledge graph data into one unified vector space which aims at preserving key information within the knowledge graph data and thus can be used to predict new relations between  entities. \emph{TransE} \cite{bordes_translating_2013} is a simple but powerful model which interprets the relationships as translations operating on the low-dimensional embeddings of the entities. However, this simple assumption has flaws when dealing with the more complex relations (which has been show in \cite{wang_knowledge_2014}). In this paper, we have proposed a novel approach that extends the \emph{TransE} model in a way by taking into account the multiple facets of each relation's various patterns for different entity pair in the knowledge graphs. Two large benchmark knowledge base datasets, Wordnet \cite{miller_wordnet:_1995} and Freebase \cite{bollacker_freebase:_2008}, have been used to perform the link prediction task. The experimental results show that the proposed algorithm is comparable to the state-of-art relational learning algorithm and has posed a significant improvement over  \emph{TransE}.
\end{abstract}

%
% The code below should be generated by the tool at
% http://dl.acm.org/ccs.cfm
% Please copy and paste the code instead of the example below. 
%

\begin{CCSXML}
<ccs2012>
<concept>
<concept_id>10010147.10010257.10010293.10010297.10010299</concept_id>
<concept_desc>Computing methodologies~Statistical relational learning</concept_desc>
<concept_significance>500</concept_significance>
</concept>
<concept>
<concept_id>10010147.10010257.10010293.10010319</concept_id>
<concept_desc>Computing methodologies~Learning latent representations</concept_desc>
<concept_significance>500</concept_significance>
</concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Statistical relational learning}
\ccsdesc[500]{Computing methodologies~Learning latent representations}


%
% End generated code
%

%
%  Use this command to print the description
%
\printccsdesc

% keyword not necessary
% \keywords{Knowledge Graphs;}


\section{Introduction}
Traditional machine learning algorithms usually work with data that are described by its feature vector of attributes. While statistical relational learning handles the multi-relational data where objects are interlinked by various relation types. Thus, the multi-relational data can be referred to as in the form of \emph{graph} whose nodes represent the entities and edges corresponds to the relationships. In particular, we consider to apply relational learning algorithms to the very large \emph{knowledge bases}, which encode real-world information by representing each fact following the Resource Description Framework (RDF) format.  Examples of knowledge bases include Wordnet \cite{miller_wordnet:_1995}, YAGO \cite{suchanek_yago:_2007}, DBpedia \cite{lehmann_dbpedialarge-scale_2015}, Freebase \cite{bollacker_freebase:_2008}, NELL \cite{betteridge_toward_2009} and the Google's Knowledge Vault project\cite{dong_knowledge_2014}.  

% ----------------Deleted-------------------------
%These knowledge bases are very useful for information retrieval, question answering and other related AI tasks.
% ----------------Deleted-------------------------

The knowledge bases are very large and usually contain millions of nodes and billions of edges. In fact, most databases are noisy and far from complete as such large databases are either constructed collaboratively by an open group of volunteers or  automatically extracted from the unstructured or semi-structured  text via certain rules, regular expressions or machine learning and natural language processing techniques \cite{weikum_information_2010}. Typical tasks of relational learning on knowledge bases include correcting the missing or invalid relations, identify the duplicate entities and perform link-based clustering. 

To work with these huge knowledge databases, there arises a promising approach which embeds the entities and relationships into a continuous vector space while subsequently geometric operations in this embedded space have been used to model the knowledge graph data. Among them, \emph{TransE} \cite{bordes_translating_2013} is a highly promising model which has achieved very well performance with only the minimal parametrisation. However, the assumption behind \emph{TransE} that  all the relationships are hierarchal  is too simple to adequately model the complex structure of  knowledge graphs. As a consequence, various other algorithms \cite{fan_transition-based_2014} \cite{wang_knowledge_2014} \cite{lin_learning_2015} \cite{garcia-duran_composing_2015}  have been proposed to extend the \emph{TransE} model to explore for a more reliable assumption. These algorithms have either extended the \emph{TransE} by weighting adjustment scheme based on each relation types  or further assumes that the operations should be conducted on the respective relation space for each relationship. 

In this paper, we have proposed a method called Translating on Pairwise Entity Space(denoted as \emph{TranPES}),  which  put the entities and relationships into a unified vector space and models the multiple facets of each relation in the knowledge graph by solely  taking the relation vector's projection on the relevant entity pair space into account. It has then been trained on a ranking objective using stochastic gradient descent and we have compared it with other methods on the two commonly used data  of WordNet \cite{miller_wordnet:_1995} and Freebase \cite{bollacker_freebase:_2008} on the link prediction task.

The remainder of this paper is organised as follows. In section \ref{review}, we briefly review the previous works which follow the knowledge graph embedding approach. The mathematical formulation for our model and the related discussion is presented in section \ref{math}. Subsequent experiments are conducted in section \ref{exp}. Finally, we draw the conclusion and discuss about the future direction in the last section.

\section{Related work} \label{review}
Recently, embedding knowledge bases into a distributed vector space have gained a great interest (see \cite{nickel_review_2015} for a review ). Embedding-based model usually design various scoring function to measure the plausibility of each observable fact in the knowledge graph. And all models explain this confidence score via the assumed embeddings of entities. Indeed, this idea of learning word embeddings  have been shown very successful in the natural language processing that such representations can store key information for each word and help to improve the performance on standard NLP tasks \cite{bengio_neural_2006}.  Depending on the representation learning methods, the embedding-based model can be classified into the following categories: tensor/matrix factorisation \cite{singh_relational_2008} \cite{nickel_three-way_2011}, Bayesian clustering framework \cite{kemp_learning_2006} \cite{sutskever_modelling_2009} and neural networks \cite{bordes_learning_2011} \cite{jenatton_latent_2012} \cite{bordes_translating_2013} \cite{socher_reasoning_2013} \cite{bordes_semantic_2014} \cite{wang_knowledge_2014} \cite{lin_learning_2015} \cite{garcia-duran_composing_2015}. We focus on the neural network model as they are highly scalable and in particular the \emph{TransE} \cite{bordes_translating_2013} algorithm. The \emph{TransE} algorithm makes a simple assumption that regards the relation type as a translation operation on the embedded entity vectors. Such a simple assumption has led to the \emph{TransE} algorithm significantly outperforms state-of-art methods on link prediction tasks. A lot of works \cite{fan_transition-based_2014} \cite{wang_knowledge_2014} \cite{lin_learning_2015} \cite{garcia-duran_composing_2015}  based on \emph{TransE} have been proposed and are working effectively. By taking the advantage of the translation operation introduced by \emph{TransE}, we propose a novel approach that limits the translation operation conducted only on the entity pair hyperplane and tries to learn to represent  different aspects of relationships into different subspaces.





\section{Proposed Model} \label{math}
The knowledge graph data $\mathcal{D}$ consists of a set of links between a fixed set of entities. Let $\mathcal{E} = \{e_1, \ldots, e_{N_e}\}$ denote the entity set and $\mathcal{R} = \{r_1, \ldots, r_{N_r}\}$ the link set.  The relationship between entities indicated by $\mathcal{D}$ are represented as relation triples such as $(e_h,r_{\ell},e_t)$, where $e_h, e_t\in \mathcal{E}$ are referred as the head  and tail, respectively, and  $r_{\ell} \in \mathcal{R}$ the link or relation type.  For example, (\emph{Champa}, \emph{formOfgoverment}, \emph{Monarchy}) is one of such relation triples which the head entity \emph{Champa} and the tail entity \emph{Monarchy} is linked by the relation type \emph{formOfgoverment}.  For the sake of convenience of discussion, we simplify the relation triple as $(h,\ell, t)$   by referring only the indices of the entities and links. Given a set of known links within $\mathcal{D}$, the goal of a relational learning task is to  infer unknown links and correct the known links to complete the graph $\mathcal{D}$. One way to solve this task is to learn an energy function $E(h, \ell, t)$ on the set of all possible triples $\mathcal{E} \times \mathcal{R} \times \mathcal{E}$ so that a triple representing a truly existing link between two entities  is assembled with a low energy, otherwise with high energy. 

%For instance, $E(h, l, t)\geq c$ indicates there exists link $r_l$ between the entities $e_h$ and $e_t$ and $E(h, l, t)<c$ otherwise, where $c$ is a threshold. 

\subsection{Energy Function}
The proposed method  \emph{TranPES}  parameterises the energy function of an input relation triple over three individual $k$-dimensional embedding vectors of its head, tail and link, as well as as a set of $k\times k$ transformation matrices $\{\textbf{P}_{ht}\}_{h,t}$ where different matrices are constructed for different head-tail entity pairs $(h,t)$. Letting $\{\bm e_i\}_{i=1}^{N_e}$ denote the entity embedding vectors and  $\{\bm r_i\}_{i=1}^{N_r}$ the link embeddings, the \emph{\emph{TranPES}} energy function is defined as the following:
\begin{equation} \label{energy}
\mathrm{E}{(h,\ell, t)} = \|\bm{e}_h +\textbf{P}_{ht} \bm{r}_{\ell}-\bm{e}_t\|_2.
\end{equation}
Apart from the $l_2$-norm, it is  possible to compute the energy based on other norms, e.g., the $l_1$-norm, $p$-norm, or other dissimilarity measures.

 Instead of freely setting the transformation matrices, $\textbf{P}_{ht}$ is restricted as a projection matrix that  gives the projection of a $k$-dimensional vector onto the space spanned by the two $k$-dimensional entity vectors $\bm{e}_h$ and $\bm{e}_t$.  Letting the columns of the $k\times 2$ matrix $\textbf{E}_{ht}$ store the two entity embedding vectors $\bm{e}_h $ and $\bm{e}_t$, the projection matrix is given by 
\begin{equation} \label{predefP}
\textbf{P}_{ht}=\textbf{E}_{ht}\left(\textbf{E}_{ht}^T \textbf{E}_{ht} \right)^{-1}\textbf{E}_{ht}^T.
\end{equation}
To avoid the square matrix $\textbf{E}_{ht}^T \textbf{E}_{ht}$ end up being singular,  Eqs. (\ref{predefP}) can be further modified by adding a  regularisation term, leading to
\begin{equation}
\textbf{P}_{ht}=\textbf{E}_{ht}\left(\textbf{E}_{ht}^T \textbf{E}_{ht} +\xi\textbf{I}\right)^{-1}\textbf{E}_{ht}^T.
\end{equation}

From the above definition of the transformation matrix $\textbf{P}_{ht}$, the orthogonal complementary $(\textbf{I} -  \textbf{P}_{ht}) \bm{r}_{\ell}$ of the projected link embedding vector  is always close to perpendicular to  $\bm{e}_h$ and $\bm{e}_t$. This can be shown by proving that the following expression is close to $\bm{0}$.
\begin{align}
 &\begin{bmatrix} \bm{e}_h^{T} \\ \bm{e}_t^{T} \end{bmatrix} (\textbf{I} -  \textbf{P}_{ht})\bm{r}_{\ell} \notag  \\
=&\textbf{E}_{ht}^{T}\left(\textbf{I} -  \textbf{P}_{ht}\right)\bm{r}_{\ell}  \notag \\
=&\textbf{E}_{ht}^{T}\left(\bm{r}_{\ell} -  \textbf{E}_{ht}\left(\textbf{E}_{ht}^{T} \textbf{E}_{ht} + \xi \textbf{I} \right)^{-1} \textbf{E}_{ht}^{T} \bm{r}_{\ell}\right)  \notag \\
=&\textbf{E}_{ht}^{T}\bm{r}_{\ell} - \left (\textbf{E}_{ht}^{T} \textbf{E}_{ht}+\xi \textbf{I} - \xi \textbf{I}\right)\left(\textbf{E}_{ht}^{T} \textbf{E}_{ht} + \xi \textbf{I}\right )^{-1} \textbf{E}_{ht}^{T} \bm{r}_{\ell} \notag \\
=& \textbf{E}_{ht}^{T}\bm{r}_{\ell} - \textbf{E}_{ht}^{T} \bm{r}_{\ell}  +  \xi \left(\textbf{E}_{ht}^{T} \textbf{E}_{ht} + \xi \textbf{I}\right )^{-1} \textbf{E}_{ht}^{T} \bm{r}_{\ell}  \notag \\
=& \xi \left(\textbf{E}_{ht}^{T} \textbf{E}_{ht} + \xi \textbf{I}\right )^{-1} \textbf{E}_{ht}^{T} \bm{r}_{\ell}   \label{temp}
\end{align}
Let the economic singular value decomposition of the $k \times 2$ matrix $\textbf{E}_{ht}$ as:
\begin{equation*}
\textbf{E}_{ht} = \textbf{U} \mathbf{\Sigma} \textbf{V}^{T}
\end{equation*}
where \textbf{U} is a $k \times 2$ real unitary matrix, $\mathbf{\Sigma}$ is a $2 \times 2$ rectangular diagonal matrix with non-negative real numbers on the diagonal, and \textbf{V} is a $2 \times 2$ real unitary matrix.
Then the Eqs. (\ref{temp}) can be further simplified as:
\begin{align}
& \xi \left(\textbf{E}_{ht}^{T} \textbf{E}_{ht} + \xi \textbf{I}\right )^{-1} \textbf{E}_{ht}^{T} \bm{r}_{\ell} \notag \\
=& \xi \left(\textbf{V} \mathbf{\Sigma} \textbf{U}^{T}  \textbf{U} \mathbf{\Sigma} \textbf{V}^{T} + \xi \textbf{I}\right )^{-1} \textbf{V} \mathbf{\Sigma} \textbf{U}^{T} \bm{r}_{\ell} \notag \\
=& \xi \left(\textbf{V} \mathbf{\Sigma}^2 \textbf{V}^{T} + \xi \textbf{I}\right )^{-1} \textbf{V} \mathbf{\Sigma} \textbf{U}^{T} \bm{r}_{\ell} \notag \\
=& \xi \textbf{V} \left(\mathbf{\Sigma}^2 + \xi \textbf{I} \right)^{-1} \textbf{V}^{T} \textbf{V} \mathbf{\Sigma} \textbf{U}^{T} \bm{r}_{\ell} \notag \\
=& \xi \textbf{V} \left(\mathbf{\Sigma}^2 + \xi \textbf{I} \right)^{-1} \mathbf{\Sigma} \textbf{U}^{T} \bm{r}_{\ell} \notag \\
=& \xi \left( \textbf{V} \begin{bmatrix} \frac{\sigma_1}{\sigma_1^2 + \xi} & 0 \\ 0 & \frac{\sigma_2}{\sigma_2^2 + \xi}   \end{bmatrix} \textbf{U}^{T} \bm{r}_{\ell}  \right)\notag \\
\approx & \bm{0}
\end{align}

Thus, adding the regularisation term $\xi \textbf{I}$ not only making the projection transformation matrices $\{\textbf{P}_{ht}\}_{h,t}$ calculable in all circumstances but also ensures that the projected link embedding vector $\textbf{P}_{ht} \bm{r}_{\ell}$ can be spanned by the corresponding head and tail entity embedding vectors.

\subsection{Training}
Given a set of known links between entities, a set of valid triples can be created from it. This set is referred as the positive triple  set, denoted as $\mathcal{D}^+$. For each positive triple $(h,l,t)\in\mathcal{D}^+$, a set of  corrupted triples can be generated by replacing either its head or tail  with a different one, given as
\begin{align*}
\small
\mathcal{D}^{-}_{h,l,t} = &  \left\{ (h^{'},\ell,t)  |  h^{'} \in \{ 1,2,\ldots, N_e\}, (h^{'},\ell,t)\notin \mathcal{D}^+ \right\} \cup \\
 &  \left\{ (h,\ell,t^{'})  |   t^{'} \in \{ 1,2,\ldots, N_e\},  (h,\ell,t^{'}) \notin \mathcal{D}^+\right\}
\end{align*}

Minimising the energy function in Eq. (\ref{energy}) parameterised on the entity and link embeddings is equivalent to the optimisation on these embedding vectors to encourage the maximum discrimination between  the positive  and negative triples. To achieve this, a margin-based ranking loss can be employed, given as
\begin{equation} \label{loss}
\small
\mathcal{L}_m = \sum_{(h,\ell,t)\in \mathcal{D}^+} \sum_{(h^{'},\ell,t^{'})\in  \mathcal{D}^{-}_{h,\ell,t}} [\gamma + \mathrm{E}(h,\ell,t)- \mathrm{E}(h^{'},\ell,t^{'}) ]_+,
\end{equation}
where $[x]_+ \triangleq \max(0,x)$ which denotes the positive part of the input number $x$, and $\gamma > 0$ is a user-set margin parameter.

A length constraint $\|\bm{e}_i\|_2 \leq 1$ for each entity embedding  is considered to prevent the training process to trivially minimise $\mathcal{L}_m$ by artificially increasing the scale of the entity embedding. This constraint can be incorporated into the cost function $\mathcal{L}_m$ as $ \sum_{i =1}^{N_e} \left[\|\bm{e}_i\|_2^2 - 1\right]_+ $.  And we also add a regularisation term  for the relational embedding vectors $\{\bm r_i\}_{i=1}^{N_r}$. This leads to the following regularised cost function:
\begin{equation} \label{loss2}
\mathcal{L} = \mathcal{L}_m  +\lambda_1 \sum_{i =1}^{N_e} \left[\|\bm{e}_i\|_2^2 - 1\right]_+ + \lambda_2 \sum_{j=1}^{N_r} \| \bm{r}_{j} \|_2^2,
\end{equation}
where $\lambda_1>0$ is the scale control parameter and $\lambda_2 >0$ is the regularisation parameter. Finally, the  following optimisation problem need to be solved, given as
\begin{equation}
\argmin_{\{\bm e_i\}_{i=1}^{N_e}, \{\bm r_i\}_{i=1}^{N_r}} \mathcal{L}\left(\{\bm e_i\}_{i=1}^{N_e}, \{\bm r_i\}_{i=1}^{N_r}, \gamma, \xi,\lambda_1, \lambda_2 \right).
\end{equation}

Pseudo code of the proposed algorithm is provided in Algorithm $\textbf{1}$. Similar to the optimisation procedure used in \cite{bordes_translating_2013}, a stochastic gradient descent algorithm in minibatch mode is used. All embedding vectors for entities and relations are first initialised following the random procedure in \cite{glorot_understanding_2010}. At each main iteration, a set of positive triples for minibatch training is randomly sampled from the training set and the corresponding corrupted triples are generated from each individual positive triple in this set. After a minibatch, the gradient is computed and the model parameters are updated. The algorithm is stopped for a given fixed number of iterations. 


\begin{algorithm*}
\caption{Learning \emph{TranPES} }
\textbf{input} Training set $\mathcal{D} = \{(h,\ell,t)\}$, entities and rel. set $\mathcal{E}$ and $\mathcal{R}$, margin $\gamma$, regularisation term $\lambda_1, \lambda_2$ and embeddings dimension $k$.
\begin{enumerate}
\item \textbf{initialise} 
\begin{description}
\item $\bm{r} \leftarrow$ uniform $(-\frac{6}{\sqrt{k}}, \frac{6}{k})$ for each $\bm{r} \in \mathcal{R}$ 
\item	$\bm{r} \leftarrow \bm{r} / \| \bm{r} \|$ for each $\bm{r} \in \mathcal{R}$
\item $\bm{e} \leftarrow$ uniform $(-\frac{6}{\sqrt{k}}, \frac{6}{k})$ for each $\bm{e} \in \mathcal{E}$ 
\item	$\bm{e} \leftarrow \bm{e} / \| \bm{e} \|$ for each $\bm{e} \in \mathcal{E}$
\end{description}

\item \textbf{loop}
\begin{description}
\item $\mathcal{D}_{batch} \leftarrow$ sample from $\mathcal{D}$ 	{\color{green} // sample a minibatch triplets of size b}
\item $\mathcal{T}_{batch} \leftarrow \emptyset$  {\color{green} //  for storing the set of pairs of correct and corrupted triplets}
\item \textbf{for} $(h,l,t) \in \mathcal{D}$ \textbf{do}
\begin{description}
\item $(h^{'},\ell,t^{'}) \leftarrow$ sample from $\mathcal{D}^{-}_{(h,l,t)}$ {\color{green} // sample a corrupted triplet}
\item $\mathcal{T}_{batch} \leftarrow \mathcal{T}_{batch} \cup \{ ((h,\ell,t),(h^{'},\ell,t^{'})) \}$
\end{description}
\item \textbf{end for}
\item $\mathcal{E}_{batch} \leftarrow$  head and tail set from $\mathcal{T}_{batch}$
\item $\mathcal{R}_{batch} \leftarrow$  link set from $\mathcal{T}_{batch}$ 
\item Updates embeddings w.r.t.  
\begin{equation*}
\sum_{(h, \ell, t),(h^{'},\ell,t^{'})\in  \mathcal{T}_{batch}} {[\gamma + \mathrm{E}(h,l,t)- \mathrm{E}(h^{'},\ell,t^{'}) ]_+}  +\lambda_1 \sum_{m \in \mathcal{E}_{batch}} \left[\|\bm{e}_m\|_2^2 - 1\right]_+ + \lambda_2 \sum_{n \in \mathcal{R}_{batch}} \| \bm{r}_{n} \|_2^2,
\end{equation*}

\end{description}
\textbf{end loop}

\end{enumerate}

\end{algorithm*}


\subsection{Discussion}
The proposed idea is inspired by the limitation of one of the most commonly used algorithm \emph{TransE} \cite{bordes_translating_2013}  in relational learning. The Energy function of this model can be expressed in the following:
\begin{equation}
\mathrm{E}{(h,\ell, t)} = \|\bm{e}_h + \bm{r}_{\ell}-\bm{e}_t\|_2^2
\end{equation}

For any triplets of the form \{$(h, r_1, t)$, $(h, r_2, t)$, \ldots, $(h, r_m, t)$\}, the \emph{TransE}  and many other algorithms may produce result that lead to all the relation types  \{ $r_1$, $r_2$, \ldots, $r_m$\} involved to be equal. As in the case of \emph{TransE} algorithm, it tends to converge all the relation vectors into the subtracting vector  $\bm{e}_h - \bm{e}_t$.  Example of such triples can find as  \emph{(Obama, presidentOf, USA)}, \emph{(Obama, placeOfbirth, USA)}.

 On the other hand, considering the simple link of ``$isa$" in  the statement \emph{Bob Dylan was a song writer, singer, performer, book author and film actor}, the following relation triples can be generated from it:
\\
\begin{tabular}[center]{l l l} 
 \\
 \emph{head} & \emph{link} & \emph{tail} \\
 \hline 
 (\emph{BobDylan}, & \emph{isa}, & \emph{SongWriter}) \\
 (\emph{BobDylan}, & \emph{isa}, & \emph{Singer}) \\
  (\emph{BobDylan}, & \emph{isa}, & \emph{Performer}) \\
   (\emph{BobDylan}, & \emph{isa}, & \emph{BookAuthor }) \\
    (\emph{BobDylan}, & \emph{isa}, & \emph{FilmActor}) \\\\
 \end{tabular}
 
Both \emph{transE} and many other existing algorithms assign one unified representation for the link $r_{\ell}$ in all triplets.  Since the learned energies of the valid triples  possess low values,  different entities such as song writer, singer and performer are  distributed very close to each other in the embedding space due to their closeness to the same embedding vector of  the link ``$isa$". Although there exist distinctions between the occupations represented by these entities, the learned embeddings are not able to highlight such distinction. To overcome this,  the proposed energy function  first projects the link embedding vector onto the hyperplane spanned by the two  entity embedding vectors, and then performs the distance computation within the hyperplane.  This subsequently results in different representations of the link when it gets involved with different entity pairs.  

The energy function  $\mathrm{E}{(h,\ell, t)} = \|\textbf{P}_l\bm{e}_h + \bm{r}_{\ell}-\textbf{P}_l\bm{e}_t\|_2^2$ of \emph{TransR} \cite{lin_learning_2015} is another one proposed to overcome the limitation of   \emph{transE}. It allows the entities and links to be distributed in different embedding spaces of dimensions $d$ and $k$, respectively, and  introduces a set of $ k\times d$ projection matrices $\{\textbf{P}_l\}_{l=1}^{N_r}$ to align the two spaces over each link. In a way, it fixes an embedding space for the links, and project each pair of the entities $(\bm{e}_h, \bm{e}_t)$ onto this space to further formulate the distance. For the same entity,  its final representation $\textbf{P}_l\bm{e}_h$ used for comparing distance  differs over different links. On the contrary, the proposed method allows various customised representations $\textbf{P}_{ht} \bm{r}_{\l}$ for a link to suit the needs of different entity pairs, but maintains the fixed embedding representation for the entities when comparing distance. This thus allows the opportunity to propagate the relation information through entities. For instance, we assume there exist entities $c_1,c_2, \ldots,d_1,d_2,\ldots, e_1, e_2, \ldots$ belonging to three classes of $C,D,E$ and the class structure can be reflected by the link information.   When the entities are projected in a same fixed space,  their embeddings are able to  show directly the within-class closeness and between-class dispersion in the same space, thus it is possible to  transfer the instance-based inference to the class-based inference e.g., from $(c_i, r_1, d_j) \wedge (d_j, r_2,, e_k) \Rightarrow  (c_i, r_3, e_k)$ to $(C, r_1, D)\wedge (D, r_2, E) \Rightarrow  (C, r_3, E)$.



\section{Experiments} \label{exp}
To compare our proposed algorithm with various other methods from the literature, we evaluate our approach on the two common used datasets from knowledge bases, Wordnet \cite{miller_wordnet:_1995} and Freebase \cite{bollacker_freebase:_2008}. We listed some statistics of these data in Table \ref{data}.

\subsection{Data sets}
\subsubsection{Wordnet} 

This is a large lexical database of English. It groups words into sets of cognitive synonyms (synsets) and interlink these synsets by means of a small number of semantic relations, such as \emph{synonymy}, \emph{antonymy}, \emph{meronymy} and \emph{hypernymy}.  For example, a typical triplet \emph{(\_range\_NN\_3, \_hypernym, \_tract\_NN\_1)} means the third meaning of the noun "range"  is a hypernym of the first sense of the noun "tract". We consider the data \emph{WN18} used in \cite{bordes_semantic_2014}, which contains 40,943 entities and 18 relational types.

\subsubsection{Freebase} 
Freebase is a massive online collection database consisting of general human knowledge. It organises the human knowledge data  directly in the triplet form of \emph{(head, link, tail)}. Unlike the Wordnet database, it does not disambiguate the senses of each entity. Typical triplet examples include \emph{(Peasant Magic, album, Solipsitalism)}, \emph{(Barack Obama, religion, Christianity)} and \emph{(Orange France, place-founded, Paris)}. FB15k \cite{bordes_learning_2011} is created by adopting the frequent occurrent entities in the Freebase database and is used for comparison with other algorithms in this paper.

\begin{table}[t]
\caption{Statistics of data sets} \label{data}
\centering
\begin{tabular}[center]{|l |l l |} 
 \hline
Dataset & WN18 & FB15k \\
 \hline 
Relationships & $18$ & $1,345$ \\
Entities & $40,943$ & $14,951$ \\
Train & $141,442$ & $483,142$ \\
Valid & $5,000$ & $50,000$ \\
Test & $5,000$ & $59,071$ \\
\hline
 \end{tabular}
 \end{table}
 
\subsection{Evaluation}
We compare the proposed algorithm with the state-of-art  methods on the WN18 and FB15k data set for link prediction. Essentially, every method trains a score function (or energy function in our case) to assemble the likely relations with higher scores (or lower energies) than the unlikely relations. This score function (energy function) can thus give its estimation of the likely score for every true triplet in the test set. Two evaluation metrics\cite{bordes_learning_2011}, \emph{mean rank} and \emph{hits@10}, about measuring how well the predicted scores correlated with the true triplets in the test set have been applied in our paper.

\begin{table*}
\caption{Evaluation Results on WN18 and FB15k} \label{result1}
\centering
\begin{tabular}[center]{| c | c  c  | c c | c   c | c  c |}  
 \hline
Dataset & \multicolumn{4}{c}{WN18} & \multicolumn{4}{|c|}{FB15k} \\
\hline
\multirow{2}{*}{Metric} & \multicolumn{2}{ c |}{Mean Rank} & \multicolumn{2}{c |}{Hits@10(\%)} & \multicolumn{2}{c |}{Mean Rank}  &  \multicolumn{2}{c|}{Hits@10(\%)}   \\ 
\multicolumn{1}{|  c |}{} 	& 
Raw & Filter & Raw & Filter& Raw & Filter &  Raw & Filter \\
\hline
Unstructured \cite{bordes_semantic_2014} & $315$ & $304$ & $35.3$ & $38.2$ & $1,074$ & $979$ & $4.5$ & $6.3$ \\
\hline
RESCAL \cite{nickel_three-way_2011} & $1,180$ & $1,163$ & $37.2$ & $52.8$ & $828$ & $683$ & $28.4$ & $44.1$ \\
\hline
SE \cite{bordes_learning_2011} & $1,011$ & $985$ & $68.5$ & $80.5$ & $273$ & $162$ & $28.8$ & $39.8$ \\
\hline
SME(linear) \cite{bordes_semantic_2014} & $545$ & $533$ & $65.1$ & $74.1$ & $274$ & $154$ & $30.7$ & $40.8$ \\
\hline
SME(bilinear) \cite{bordes_semantic_2014}  & $526$ & $509$ & $54.7$ & $61.3$ & $284$ & $158$ & $31.3$ & $41.3$ \\
\hline
LFM \cite{jenatton_latent_2012}  & $469$ & $456$ & $71.4$ & $81.6$ & $283$ & $164$ & $26.0$ & $33.1$ \\
\hline
TransE \cite{bordes_translating_2013} & $263$ & $251$ & $75.4$ & $89.2$ & $243$ & $125$ & $34.9$ & $47.1$ \\
\hline
TransH \cite{wang_knowledge_2014} & $318$ & $303$ & $75.4$ & $86.7$ & $211$ & $84$ & $42.5$ & $58.5$ \\
\hline
TransR \cite{lin_learning_2015} & $232$ & $219$ & $78.3$ & $91.7$ & $226$ & $78$ & $43.8$ & $65.5$ \\
\hline
CTransR \cite{lin_learning_2015} & $243$ & $230$ & $\mathbf{78.9}$ & $\mathbf{92.3}$ & $233$ & $82$ & $44$ & $66.3$ \\
\hline
\emph{TranPES} & $\mathbf{223}$ & $\mathbf{212}$ & $71.6$ & $81.3$ & $\mathbf{198}$ & $\mathbf{66}$ & $\mathbf{48.05}$ & $\mathbf{67.3}$ \\
\hline
\end{tabular}

\quad
\caption{Detailed Evaluation on FB15k} \label{result2}

\centering
\begin{tabular}{| c | c  c  | c c | c   c | c  c |} 
 \hline
Tasks & \multicolumn{4}{ c |}{Predicting Head(Hits@10)} & \multicolumn{4}{c |}{Predicting Tail(Hits@10)}  \\ 
\hline
Relation Category	& 
1-To-1 & 1-To-M & M-To-1 & M-To-M& 1-To-1 & 1-To-M &  M-To-1 & M-To-M \\
\hline
Unstructured \cite{bordes_semantic_2014} & $34.5$ & $2.5$ & $6.1$ & $6.6$ & $34.3$ & $4.2$ & $1.9$ & $6.6$ \\
\hline
SE \cite{bordes_learning_2011} & $35.6$ & $62.6$ & $17.2$ & $37.5$ & $34.9$ & $14.6$ & $68.3$ & $41.3$ \\
\hline
SME(linear) \cite{bordes_semantic_2014} & $35.1$ & $53.7$ & $19.0$ & $40.3$ & $32.7$ & $14.9$ & $61.6$ & $43.3$ \\
\hline
SME(bilinear) \cite{bordes_semantic_2014}  & $30.9$ & $69.6$ & $19.9$ & $38.6$ & $28.2$ & $13.1$ & $76.0$ & $41.8$ \\
\hline
TransE \cite{bordes_translating_2013} & $43.7$ & $65.7$ & $18.2$ & $47.2$ & $43.7$ & $19.7$ & $66.7$ & $50.0$ \\
\hline
TransH \cite{wang_knowledge_2014} & $66.7$ & $81.7$ & $30.2$ & $57.4$ & $63.7$ & $30.1$ & $83.2$ & $60.8$ \\
\hline
TransR \cite{lin_learning_2015} & $76.9$ & $77.9$ & $38.1$ & $66.9$ & $76.2$ & $38.4$ & $76.2$ & $69.1$ \\
\hline
CTransR \cite{lin_learning_2015} & $\mathbf{78.6}$ & $77.8$ & $36.4$ & $\mathbf{68.0}$ & $77.4$ & $37.8$ & $78.0$ & $\mathbf{70.3}$ \\
\hline
\emph{TranPES} & $78.0$ & $\mathbf{88.6}$ & $\mathbf{38.9}$ & $67.3$ & $\mathbf{78.9}$ & $\mathbf{42.1}$ & $\mathbf{84.2}$ & $69.8$ \\
\hline
\end{tabular}

\end{table*}

\subsubsection{mean rank}
 For each correct triplet in the test set, we first construct the corrupt triples by replacing the head entity with all the entities in knowledge base in turn. The scores (or energies) of these corrupted triples are computed by each model and have been sorted in descending (or ascending) order. The rank for this correct head entity is finally stored. This procedure is repeated by replacing the tail entity of each correct triple with all the entities in the dictionary to obtain the rank for each correct tail entity. The \emph{mean rank} of all these predicted ranks  in the test set has been used for the assessment.

\subsubsection{hits@10} 
Apart from utilising the \emph{mean rank} evaluation scheme, we also report the proportion of the correct entities ranked within top $10$  as a supplemental evaluation indicator. During the model training, we notice that the margin $\gamma$ highly correlated to the $hits@10$ performance. In general, smaller $\gamma$ value will result in higher $hits@10$ score.

\subsubsection{filtered mean rank and hits@10}
The ranking of each entity within these evaluation metrics can be inaccurate when some corrupted triples turn out to be the correct ones in the knowledge graph. \cite{bordes_translating_2013} suggests that we should filter out the corrupted triples which appear either in the training, validation or test set in the ranking procedure.  We label the first evaluation metric as \emph{raw} and the later filtered evaluation metric as \emph{filtered}.


\subsection{Results}
In the experimental phase, we direct compare our proposed models with those results reported in the literature  \cite{lin_learning_2015} as we are using the same data sets. For the experiments of the \emph{TranPES} algorithm, we search the learning rate among $\{ 0.1, 0.01, 0.002\}$, the dimension of the entity and link embedding $k$ among $\{ 20, 50, 100 \}$, the scaling control factor $\lambda_1$ is assigned as a constant value $1$, batch size $B$ among $\{50, 100, 200\}$, the regularisation parameter $\lambda_2$ among $\{ 1,$ $0.1,$ $0.01\}$ and the margin $\gamma$ among \{0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0\}. The value of margin $\gamma$ is closely related to the $hits@10$ evaluation metric thus it is important to do a full searching for this parameter. For both datasets, we limit the epochs round at most 1000 times.  And we select the best model among the last $100$ epochs according to the best \emph{mean rank} on the validation set. An open-source implementation of \emph{TranPES} is available from the project webpage\footnote{\url{https://github.com/while519/TranPES.git}}.

The optimal configurations are: $k=20, B = 100, \lambda_2=0.01, \gamma=0.7$ on WN18 and $k=100, B=100, \lambda_2 = 0.01, \gamma=0.4$ on FB15k. The evaluation results on two datasets are present on Table \ref{result1}.

On the WN18 data set, it seems that the proposed algorithm does not perform better than its simpler form of \emph{TransE} algorithm in terms of the $hits@10$ evaluation, this might be caused by the small number of relations in this data. While on the more complex data set FB15k, which has $1,345$  relation types, the \emph{TranPES} outperforms all the other algorithms.  We could see that the $hit@10$ results of \emph{TransR} and \emph{TranPES} is close on the FB15k database that the \emph{TranPES} has a filtered $hits@10$ of $67.3\%$ compared to $66.3\%$ of the \emph{TransR}.   But  considering the number of parameters, our \emph{TranPES} algorithm has much less parameters for the training (only $O(N_ek+N_rk)$ which is as the same number of parameters as \emph{TransE}). And in the experiment, our algorithm is around 3 times to the \emph{TransE} algorithm.

We follow the same detailed evaluation protocol proposed in \cite{bordes_translating_2013}. It classifies the $hits@10$ results according to the four categories for each relationship:  1-To-1, 1-To-Many, Many-To-1, Many-To-Many. For instance, a given relationship is classified in to 1-To-Many if a \emph{head} can appear with at many \emph{tails} and a \emph{tail} can appear with at most one \emph{head}.  The corresponding result was shown on Table \ref{result2}. 

It can be seen that our proposed algorithm consistently outperform all other algorithms except the Cluster-based \emph{TransR} algorithm which has a comparable performance to ours.  And as one would expect, it is more approachable to predict \emph{head} in the 1-To-1, 1-To-Many  relationships and predict \emph{tail} in the 1-To-1, Many-To-1 relationship. This has been shown on the \emph{TranPES} results correspondingly. Upon successfully predicting these relationships (above $75\%$ are predicted within top $10$ rank), it also proves the reasoning ability of \emph{TranPES} algorithm over the knowledge graph.




\section{Conclusion and Future Work} \label{conclusion}
We present a new relational latent feature model for embedding the objects (e.g. \emph{relations}, \emph{entities}) of the multi-relational data into a semantic vector space.  Thus, the operations (translation, projection .etc) between the entity vectors in this space have been used to model the interlinkages within the  data. The developed algorithm, named $\emph{TranPES}$, can be viewed as an extension to the existing \emph{TransE} algorithm. It allows various  representations for a single relation to suit the need of different entity pairs and employs a fixed embedding representation for the entities  to permit  information propagation within the knowledge graphs.  Comparing our approach with the existing algorithms on the large-scale FB15k knowledge database indicate that the proposed algorithm finds a better trade-offs between model complexity and model accuracy.

Our idea was developed to increase the expressivity of the simpler \emph{TransE} model to deal with  more complex relations in the knowledge graphs. For example, training the simple triplets of \emph{(Obama, placeOfbirth, USA)} and \emph{(Obama, presidentOf, USA)} would result in the identical embedding representation of the two indeed different relations \emph{placeOfbirth} and \emph{presidentOf}  in the \emph{TransE} model. 

In the experiment, we have shown the powerful ability of statistical relational learning model for the link prediction task, especially when predicting \emph{head}  in 1-To-1, 1-To-Many relationships  and predicting \emph{tail} in 1-To-1, Many-To-1 relationships. But the successful of this reasoning mechanism within the knowledge graph has not been unveiled. Future work might focus on  learning the latent feature model as well as giving clues on how each existing triplet support in predicting new triples.

\section*{Acknowledgment}
This research has been supported by a PhD studentship jointly funded from the University of Liverpool and the China Scholarships Council.


\bibliographystyle{abbrv}
\bibliography{My_Library}

\balancecolumns

\end{document} 